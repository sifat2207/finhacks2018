i=1
paste0("feature_",i)
model_glm=glm(formu,
data=data_train%>%select_(.dots=c(vardep,list_features[[paste0("feature_",i)]])),
family = binomial(link="logit"))
model_glm
list_model=list()
list_model=list()
threshold <- 0.5
for (i in c(1:3)){
model_glm=glm(formu,
data=data_train%>%select_(.dots=c(vardep,list_features[[paste0("feature_",i)]])),
family = binomial(link="logit"))
#validitas
data_train$pred <- predict(model, type="response", newdata=data_train)
# Calculate the area under the ROC curve
roc.curve <- roc(data_train$flag_kredit_macet, data_train$pred, ci=T)
# Calculates a cross-tabulation of observed and predicted classes
# with associated statistics
con=confusionMatrix(factor(data_train$pred>threshold), factor(data_train$flag_kredit_macet==1), positive="TRUE")
list_temp=list()
list_temp[["model"]]=model_glm
list_temp[["ROC"]]=roc.curve
list_temp[["confusionmatrix"]]=con
list_model[[paste0("Model_",i)]]=list_temp
}
i
source('E:/Sifat/finhacks2018/npl-20180815/npl.R')
i=2
model_glm=glm(formu,
data=data_train%>%select_(.dots=c(vardep,list_features[[paste0("feature_",i)]])),
family = binomial(link="logit"))
data_train$pred <- predict(model, type="response", newdata=data_train)
roc.curve <- roc(data_train$flag_kredit_macet, data_train$pred, ci=T)
con=confusionMatrix(factor(data_train$pred>threshold), factor(data_train$flag_kredit_macet==1), positive="TRUE")
list_temp=list()
list_temp[["model"]]=model_glm
list_temp[["ROC"]]=roc.curve
list_temp[["confusionmatrix"]]=con
i
model_glm=glm(formu,
data=data_train%>%select_(.dots=c(vardep,list_features[[paste0("feature_",i)]])),
family = binomial(link="logit"))
list_model=list()
threshold <- 0.5
for (i in c(1:3)){
model_glm=glm(formu,
data=data_train%>%select_(.dots=c(vardep,list_features[[paste0("feature_",i)]])),
family = binomial(link="logit"))
#validitas
data_train$pred <- predict(model_glm, type="response", newdata=data_train)
# Calculate the area under the ROC curve
roc.curve <- roc(data_train$flag_kredit_macet, data_train$pred, ci=T)
# Calculates a cross-tabulation of observed and predicted classes
# with associated statistics
con=confusionMatrix(factor(data_train$pred>threshold), factor(data_train$flag_kredit_macet==1), positive="TRUE")
list_temp=list()
list_temp[["model"]]=model_glm
list_temp[["ROC"]]=roc.curve
list_temp[["confusionmatrix"]]=con
list_model[[paste0("Model_",i)]]=list_temp
}
i
list_model
list_model
install.packages("xgboost")
library(xgboost)
var_nominal
"kode_cabang" %in% list_features[["feature_1"]]
"kode_cabang" !%in% list_features[["feature_1"]]
!"kode_cabang" %in% list_features[["feature_1"]]
list_features[["feature_1"]][!var_nominal %in% list_features[["feature_1"]]]
!var_nominal %in% list_features[["feature_1"]]
list_features[["feature_1"]][var_nominal %in% list_features[["feature_1"]]]
list_features[["feature_1"]][var_nominal %in% list_features[["feature_1"]]]
var_nominal %in% list_features[["feature_1"]]
list_features[["feature_1"]][list_features[["feature_1"]]==var_nominal]
list_features[["feature_1"]]==var_nominal
list_features[["feature_1"]]%in%var_nominal
list_features[["feature_1"]]%in%!var_nominal
1list_features[["feature_1"]]%in%var_nominal
!list_features[["feature_1"]]%in%var_nominal
list_features[["feature_1"]][!list_features[["feature_1"]]%in%var_nominal]
var_numerik=list_features[["feature_1"]][!list_features[["feature_1"]]%in%var_nominal]
list_features[["feature_1"]]
i=1
var_nominal %in% list_features[[paste0("feature_",i)]]
"kode_cabang" %in% list_features[[paste0("feature_",i)]]
region <- model.matrix(~kode_cabang-1,data_train)
region
delikuensi <- model.matrix(~skor_delikuensi-1,data_train)
delikuensi
i=1
var_numerik=list_features[[paste0("feature_",i)]][!list_features[[paste0("feature_",i)]]%in%var_nominal]
data_numerik=data_train%>%select_(.dots=c(vardep,var_numerik))
if(var_nominal %in% list_features[[paste0("feature_",i)]]){
if("kode_cabang" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
region <- model.matrix(~kode_cabang-1,data_train)
data_numerik=cbind(data_numerik,region)
}
if("skor_delikuensi" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
delikuensi <- model.matrix(~skor_delikuensi-1,data_train)
data_numerik=cbind(data_numerik,delikuensi)
}
}
var_numerik=list_features[[paste0("feature_",i)]][!list_features[[paste0("feature_",i)]]%in%var_nominal]
data_numerik=data_train%>%select_(.dots=c(vardep,var_numerik))
if("kode_cabang" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
region <- model.matrix(~kode_cabang-1,data_train)
data_numerik=cbind(data_numerik,region)
}
if("skor_delikuensi" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
delikuensi <- model.matrix(~skor_delikuensi-1,data_train)
data_numerik=cbind(data_numerik,delikuensi)
}
data_matrix <- data.matrix(data_numerik)
data_matrix
var_numerik=list_features[[paste0("feature_",i)]][!list_features[[paste0("feature_",i)]]%in%var_nominal]
data_numerik=data_train%>%select_(.dots=c(var_numerik))
if("kode_cabang" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
region <- model.matrix(~kode_cabang-1,data_train)
data_numerik=cbind(data_numerik,region)
}
if("skor_delikuensi" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
delikuensi <- model.matrix(~skor_delikuensi-1,data_train)
data_numerik=cbind(data_numerik,delikuensi)
}
data_matrix <- data.matrix(data_numerik)
dtrain <- xgb.DMatrix(data = data_matrix, label= data_train%>%select_(.dots=c(vardep)))
nrow(data_matrix)
train_label=data_train%>%select_(.dots=c(vardep))
nrow(train_label)
dtrain <- xgb.DMatrix(data = data_matrix, label=train_label )
train_label=data.matrix(data_train%>%select_(.dots=c(vardep)))
dtrain <- xgb.DMatrix(data = data_matrix, label=train_label )
dtrain
model <- xgboost(data = dtrain, # the data
nround = 2, # max number of boosting iterations
objective = "binary:logistic")  # the objective function
model
predict(model)
model_xgboost <- xgboost(data = dtrain, # the data
nround = 2, # max number of boosting iterations
objective = "binary:logistic")  # the objective function
data_train$pred <- predict(model_xgboost, dtrain)
roc.curve <- roc(data_train$flag_kredit_macet, data_train$pred, ci=T)
source('E:/Sifat/finhacks2018/npl-20180815/npl.R')
list_model
names(list_model)
list_model
list_model$Model_4
list_model$Model_6
list_model$Model_6$confusionmatrix
list_model$Model_4$confusionmatrix
?rm
source('E:/Sifat/finhacks2018/npl-20180815/npl.R')
list_model
data_fraud=read.csv(choose.files())
nrow(data_fraud)
names(data_fraud)
table(data_fraud$flag_transaksi_finansial)
table(data_fraud$flag_transaksi_fraud)
prop.table(table(data_fraud$flag_transaksi_fraud))
devtools::install_github('catboost/catboost', subdir = 'catboost/R-package')
SMOTE <- function(form,data,
perc.over=200,k=5,
perc.under=200,
learner=NULL,...
)
# INPUTS:
# form a model formula
# data the original training set (with the unbalanced distribution)
# minCl  the minority class label
# per.over/100 is the number of new cases (smoted cases) generated
#              for each rare case. If perc.over < 100 a single case
#              is generated uniquely for a randomly selected perc.over
#              of the rare cases
# k is the number of neighbours to consider as the pool from where
#   the new examples are generated
# perc.under/100 is the number of "normal" cases that are randomly
#                selected for each smoted case
# learner the learning system to use.
# ...  any learning parameters to pass to learner
{
# the column where the target variable is
tgt <- which(names(data) == as.character(form[[2]]))
minCl <- levels(data[,tgt])[which.min(table(data[,tgt]))]
# get the cases of the minority class
minExs <- which(data[,tgt] == minCl)
# generate synthetic cases from these minExs
if (tgt < ncol(data)) {
cols <- 1:ncol(data)
cols[c(tgt,ncol(data))] <- cols[c(ncol(data),tgt)]
data <-  data[,cols]
}
newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
if (tgt < ncol(data)) {
newExs <- newExs[,cols]
data <- data[,cols]
}
# get the undersample of the "majority class" examples
selMaj <- sample((1:NROW(data))[-minExs],
as.integer((perc.under/100)*nrow(newExs)),
replace=T)
# the final data set (the undersample+the rare cases+the smoted exs)
newdataset <- rbind(data[selMaj,],data[minExs,],newExs)
# learn a model if required
if (is.null(learner)) return(newdataset)
else do.call(learner,list(form,newdataset,...))
}
source('E:/Sifat/Amartha/Progress/ACILES/Update ACILES/CS/Amartha-r-cstrip-d2a75ea7b3ae/smote function.R')
library(dplyr)
data_smote <- SMOTE(flag_kredit_macet ~ ., data_train%>%select_(.dots=c(vardep,list_features[["feature_1"]])),
perc.over = 200, perc.under=300)
data_train%>%select_(.dots=c(vardep,list_features[["feature_1"]]))
data_smote <- SMOTE(flag_kredit_macet ~ .,
data_train%>%select_(.dots=c(vardep,list_features[["feature_2"]])),
perc.over = 200, perc.under=300)
library(DMwR)
data_smote <- SMOTE(flag_kredit_macet ~ .,
data_train%>%select_(.dots=c(vardep,list_features[["feature_2"]])),
perc.over = 200, perc.under=300)
data_train%>%select_(.dots=c(vardep,list_features[["feature_2"]]))%>%str()
SMOTE(Species ~ .,iris,'setosa',perc.under=400,perc.over=300,
learner='svm',gamma=0.001,cost=100)
SMOTE(Species ~ .,iris,'setosa',perc.under=400,perc.over=300,
learner='svm',gamma=0.001,cost=100)
library(DMwR)
ms <- SMOTE(Species ~ .,iris,'setosa',perc.under=400,perc.over=300,
learner='svm',gamma=0.001,cost=100)
newds <- SMOTE(Species ~ .,iris,'setosa',perc.under=300,k=3,perc.over=400)
form=Species ~ .
data=iris
tgt <- which(names(data) == as.character(form[[2]]))
tgt
form[[2]]
minCl <- levels(data[,tgt])[which.min(table(data[,tgt]))]
minCl
minExs <- which(data[,tgt] == minCl)
if (tgt < ncol(data)) {
cols <- 1:ncol(data)
cols[c(tgt,ncol(data))] <- cols[c(ncol(data),tgt)]
data <-  data[,cols]
}
data
newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
perc.under=400
perc.over=300
if (tgt < ncol(data)) {
cols <- 1:ncol(data)
cols[c(tgt,ncol(data))] <- cols[c(ncol(data),tgt)]
data <-  data[,cols]
}
newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
k=5
newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
if (tgt < ncol(data)) {
newExs <- newExs[,cols]
data <- data[,cols]
}
selMaj <- sample((1:NROW(data))[-minExs],
as.integer((perc.under/100)*nrow(newExs)),
replace=T)
newdataset <- rbind(data[selMaj,],data[minExs,],newExs)
if (is.null(learner)) return(newdataset)
else do.call(learner,list(form,newdataset,...))
tgt <- which(names(data) == as.character(form[[2]]))
minCl <- levels(data[,tgt])[which.min(table(data[,tgt]))]
minExs <- which(data[,tgt] == minCl)
if (tgt < ncol(data)) {
cols <- 1:ncol(data)
cols[c(tgt,ncol(data))] <- cols[c(ncol(data),tgt)]
data <-  data[,cols]
}
newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
if (tgt < ncol(data)) {
newExs <- newExs[,cols]
data <- data[,cols]
}
selMaj <- sample((1:NROW(data))[-minExs],
as.integer((perc.under/100)*nrow(newExs)),
replace=T)
newdataset <- rbind(data[selMaj,],data[minExs,],newExs)
newdataset
smote.exs <- function(data,tgt,N,k)
# INPUTS:
# data are the rare cases (the minority "class" cases)
# tgt is the name of the target variable
# N is the percentage of over-sampling to carry out;
# and k is the number of nearest neighours to use for the generation
# OUTPUTS:
# The result of the function is a (N/100)*T set of generated
# examples with rare values on the target
{
nomatr <- c()
T <- matrix(nrow=dim(data)[1],ncol=dim(data)[2]-1)
for(col in seq.int(dim(T)[2]))
if (class(data[,col]) %in% c('factor','character')) {
T[,col] <- as.integer(data[,col])
nomatr <- c(nomatr,col)
} else T[,col] <- data[,col]
if (N < 100) { # only a percentage of the T cases will be SMOTEd
nT <- NROW(T)
idx <- sample(1:nT,as.integer((N/100)*nT))
T <- T[idx,]
N <- 100
}
p <- dim(T)[2]
nT <- dim(T)[1]
ranges <- apply(T,2,max)-apply(T,2,min) # nilai max-min dari setiap variabel
nexs <-  as.integer(N/100) # this is the number of artificial exs generated
# for each member of T
new <- matrix(nrow=nexs*nT,ncol=p)    # the new cases
for(i in 1:nT) {
# the k NNs of case T[i,]
xd <- scale(T,T[i,],ranges)
for(a in nomatr) xd[,a] <- xd[,a]==0
dd <- drop(xd^2 %*% rep(1, ncol(xd)))
kNNs <- order(dd)[2:(k+1)]
for(n in 1:nexs) {
# select randomly one of the k NNs
neig <- sample(1:k,1)
ex <- vector(length=ncol(T))
# the attribute values of the generated case
difs <- T[kNNs[neig],]-T[i,]
new[(i-1)*nexs+n,] <- T[i,]+runif(1)*difs
for(a in nomatr)
new[(i-1)*nexs+n,a] <- c(T[kNNs[neig],a],T[i,a])[1+round(runif(1),0)]
}
}
newCases <- data.frame(new)
for(a in nomatr)
newCases[,a] <- factor(newCases[,a],levels=1:nlevels(data[,a]),labels=levels(data[,a]))
newCases[,tgt] <- factor(rep(data[1,tgt],nrow(newCases)),levels=levels(data[,tgt]))
colnames(newCases) <- colnames(data)
newCases
}
newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
newExs
ms <- SMOTE(Species ~ .,iris,'setosa',perc.under=400,perc.over=300,
learner='svm',gamma=0.001,cost=100)
ms <- SMOTE(Species ~ .,iris,perc.under=400,perc.over=300,
learner='svm',gamma=0.001,cost=100)
ms <- SMOTE(Species ~ .,iris,perc.under=400,perc.over=300)
ms <- SMOTE(Species ~ .,iris,perc.under=400,perc.over=600)
data_smote <- SMOTE(flag_kredit_macet ~ .,
data_train%>%select_(.dots=c(vardep,list_features[["feature_2"]])),
perc.over = 200, perc.under=300)
data=data_train%>%select_(.dots=c(vardep,list_features[["feature_2"]]))
form=flag_kredit_macet ~ .
tgt <- which(names(data) == as.character(form[[2]]))
tgt
form
minCl <- levels(data[,tgt])[which.min(table(data[,tgt]))]
minCl
names(data)
form[[2]]
as.character(form[[2]]))
as.character(form[[2]])
names(data) == as.character(form[[2]])
which(names(data) == as.character(form[[2]]))
levels(data[,tgt])
data$flag_kredit_macet=as.character(data$flag_kredit_macet)
minCl <- levels(data[,tgt])[which.min(table(data[,tgt]))]
minCl
levels(data[,tgt])
class(data[,tgt])
class(iris[,tgt])
levels(iris$Species)
class(iris$Species)
data$flag_kredit_macet=as.factor(data$flag_kredit_macet)
tgt <- which(names(data) == as.character(form[[2]]))
minCl <- levels(data[,tgt])[which.min(table(data[,tgt]))]
tgt
minCl
minExs <- which(data[,tgt] == minCl)
if (tgt < ncol(data)) {
cols <- 1:ncol(data)
cols[c(tgt,ncol(data))] <- cols[c(ncol(data),tgt)]
data <-  data[,cols]
}
newExs <- smote.exs(data[minExs,],ncol(data),perc.over,k)
if (tgt < ncol(data)) {
newExs <- newExs[,cols]
data <- data[,cols]
}
selMaj <- sample((1:NROW(data))[-minExs],
as.integer((perc.under/100)*nrow(newExs)),
replace=T)
newdataset <- rbind(data[selMaj,],data[minExs,],newExs)
if (is.null(learner)) return(newdataset)
data_smote <- SMOTE(flag_kredit_macet ~ .,
data=data,
perc.over = 200, perc.under=300)
data=data_train%>%select_(.dots=c(vardep,list_features[["feature_1"]]))
data$flag_kredit_macet=as.character(data$flag_kredit_macet)
data$flag_kredit_macet=as.factor(data$flag_kredit_macet)
data_smote <- SMOTE(flag_kredit_macet ~ .,
data=data,
perc.over = 200, perc.under=300)
table(data_train$flag_kredit_macet)
table(data_smote$flag_kredit_macet)
data_smote$flag_kredit_macet=as.character(data_smote$flag_kredit_macet)
data_smote$flag_kredit_macet=as.numeric(data_smote$flag_kredit_macet)
table(data_smote$kode_cabang)
table(data_train$kode_cabang)
region_smote <- model.matrix(~kode_cabang-1,data_smote)
delikuensi_smote <- model.matrix(~skor_delikuensi-1,data_smote)
data_numerik_smote=cbind(data_numerik_smote,region_smote,delikuensi_smote)
data_numerik_smote=data_smote%>%select_(.dots=c(var_numerik))
data_numerik_smote=cbind(data_numerik_smote,region_smote,delikuensi_smote)
data_matrix_smote <- data.matrix(data_numerik_smote)
train_label_smote=data.matrix(data_smote%>%select_(.dots=c(vardep)))
dtrain_smote <- xgb.DMatrix(data = data_matrix_smote, label=train_label_smote )
library(xgboost)
dtrain_smote <- xgb.DMatrix(data = data_matrix_smote, label=train_label_smote )
model_xgboost_smote <- xgboost(data = dtrain_smote, # the data
nround = 2, # max number of boosting iterations
objective = "binary:logistic")  # the objective function
data_smote$pred <- predict(model_xgboost_smote, dtrain_smote)
roc.curve <- roc(data_smote$flag_kredit_macet, data_smote$pred, ci=T)
library(pROC)
roc.curve <- roc(data_smote$flag_kredit_macet, data_smote$pred, ci=T)
roc.curve
list_model$Model_4$ROC
con=confusionMatrix(factor(data_smote$pred>threshold), factor(data_smote$flag_kredit_macet==1), positive="TRUE")
library(pROC)
library(caret)
con=confusionMatrix(factor(data_smote$pred>threshold), factor(data_smote$flag_kredit_macet==1), positive="TRUE")
con
list_model$Model_4$confusionmatrix
i=1
var_numerik=list_features[[paste0("feature_",i)]][!list_features[[paste0("feature_",i)]]%in%var_nominal]
data_numerik=data_train%>%select_(.dots=c(var_numerik))
if("kode_cabang" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
region <- model.matrix(~kode_cabang-1,data_train)
data_numerik=cbind(data_numerik,region)
}
if("skor_delikuensi" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
delikuensi <- model.matrix(~skor_delikuensi-1,data_train)
data_numerik=cbind(data_numerik,delikuensi)
}
data_matrix <- data.matrix(data_numerik)
train_label=data.matrix(data_train%>%select_(.dots=c(vardep)))
dtrain <- xgb.DMatrix(data = data_matrix, label=train_label )
data_train$pred <- predict(model_xgboost_smote, dtrain)
dtrain
dtrain_smote
var_numerik
var_numerik=list_features[[paste0("feature_",1)]][!list_features[[paste0("feature_",1)]]%in%var_nominal]
data_numerik=data_train%>%select_(.dots=c(var_numerik))
if("kode_cabang" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
region <- model.matrix(~kode_cabang-1,data_train)
data_numerik=cbind(data_numerik,region)
}
if("skor_delikuensi" %in% list_features[[paste0("feature_",i)]]){
# convert categorical factor into one-hot encoding
delikuensi <- model.matrix(~skor_delikuensi-1,data_train)
data_numerik=cbind(data_numerik,delikuensi)
}
data_matrix <- data.matrix(data_numerik)
train_label=data.matrix(data_train%>%select_(.dots=c(vardep)))
dtrain <- xgb.DMatrix(data = data_matrix, label=train_label )
set.seed(1234)
data_smote=data_train%>%select_(.dots=c(vardep,list_features[["feature_1"]]))
data_smote$flag_kredit_macet=as.character(data_smote$flag_kredit_macet)
data_smote$flag_kredit_macet=as.factor(data_smote$flag_kredit_macet)
data_smote <- SMOTE(flag_kredit_macet ~ .,
data=data_smote,
perc.over = 200, perc.under=300)
data_smote$flag_kredit_macet=as.character(data_smote$flag_kredit_macet)
data_smote$flag_kredit_macet=as.numeric(data_smote$flag_kredit_macet)
region_smote <- model.matrix(~kode_cabang-1,data_smote)
delikuensi_smote <- model.matrix(~skor_delikuensi-1,data_smote)
data_numerik_smote=data_smote%>%select_(.dots=c(var_numerik))
data_numerik_smote=cbind(data_numerik_smote,region_smote,delikuensi_smote)
data_matrix_smote <- data.matrix(data_numerik_smote)
train_label_smote=data.matrix(data_smote%>%select_(.dots=c(vardep)))
dtrain_smote <- xgb.DMatrix(data = data_matrix_smote, label=train_label_smote )
model_xgboost_smote <- xgboost(data = dtrain_smote, # the data
nround = 2, # max number of boosting iterations
objective = "binary:logistic")  # the objective function
dtrain_smote
dtrain
data_smote$pred <- predict(model_xgboost_smote, dtrain_smote)
roc.curve <- roc(data_smote$flag_kredit_macet, data_smote$pred, ci=T)
con=confusionMatrix(factor(data_smote$pred>threshold), factor(data_smote$flag_kredit_macet==1), positive="TRUE")
con
roc.curve
data_train$pred <- predict(model_xgboost_smote, dtrain)
roc.curve_test <- roc(data_train$flag_kredit_macet, data_train$pred, ci=T)
roc.curve_test
con_test=confusionMatrix(factor(data_train$pred>threshold), factor(data_train$flag_kredit_macet==1), positive="TRUE")
con_test
con_test
roc.curve_test
list_model$Model_6$ROC
list_model$Model_6$confusionmatrix
109(109+14025)
109/(109+14025)
248/(248+1111)
roc.curve_test
roc.curve
